{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tuning Stable Diffusion XL with DreamBooth and LoRA\n",
    "\n",
    "This notebook demonstrates the complete pipeline for fine-tuning Stable Diffusion XL using DreamBooth and LoRA techniques. This project was developed as part of a school assignment to explore and understand modern diffusion models.\n",
    "\n",
    "## Project Overview\n",
    "\n",
    "This notebook will guide you through:\n",
    "\n",
    "1. Environment setup and dependency installation\n",
    "2. Dataset preparation requirements\n",
    "3. Model fine-tuning using DreamBooth and LoRA\n",
    "4. Inference with the fine-tuned model\n",
    "5. Creating an interactive interface\n",
    "6. (Experimental) Cloud deployment with Modal.com\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial Setup and GPU Check\n",
    "\n",
    "First, let's verify our GPU setup:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "print(f\"CUDA Available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU Device: {torch.cuda.get_device_name()}\")\n",
    "    print(\n",
    "        f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installing Required Packages\n",
    "\n",
    "Clone and install the diffusers repository:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/huggingface/diffusers\n",
    "%cd diffusers\n",
    "!pip install -e ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install DreamBooth requirements:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd examples/dreambooth\n",
    "!pip install -r requirements_sdxl.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure accelerate:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!accelerate config default\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hugging Face Authentication\n",
    "\n",
    "Set up authentication with Hugging Face:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "# IMPORTANT: Replace YOUR_TOKEN with your actual token when running\n",
    "# login(\"YOUR_TOKEN\")\n",
    "\n",
    "# Never commit or share your token!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Requirements\n",
    "\n",
    "Before proceeding with training, ensure your dataset meets these requirements:\n",
    "\n",
    "- 10-25 high-quality images\n",
    "- Good variety in angles, lighting, and poses\n",
    "- Clear, focused images without blur\n",
    "- Consistent subject matter\n",
    "\n",
    "Here's a helper script to prepare your dataset metadata:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Create metadata with all images\n",
    "metadata = {\n",
    "    \"file_name\": [\n",
    "        \"image_1.jpg\",\n",
    "        \"image_2.jpg\",\n",
    "        \"image_3.jpg\",\n",
    "        # Add all your image files\n",
    "    ],\n",
    "    \"caption\": [\n",
    "        \"image_1.txt\",\n",
    "        \"image_2.txt\",\n",
    "        \"image_3.txt\",\n",
    "        # Add corresponding captions\n",
    "    ],\n",
    "}\n",
    "\n",
    "# Create directory structure\n",
    "os.makedirs(\"dataset\", exist_ok=True)\n",
    "os.makedirs(\"dataset/images\", exist_ok=True)\n",
    "\n",
    "# Convert to DataFrame\n",
    "df = pd.DataFrame(metadata)\n",
    "\n",
    "# Save metadata\n",
    "df.to_csv(\"dataset/metadata.csv\", index=False)  # Human readable\n",
    "df.to_parquet(\"dataset/metadata.parquet\")  # For HuggingFace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Fine-tuning\n",
    "\n",
    "Now we'll fine-tune the model using DreamBooth and LoRA:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!accelerate launch train_dreambooth_lora_sdxl.py \\\n",
    "--pretrained_model_name_or_path=\"stabilityai/stable-diffusion-xl-base-1.0\" \\\n",
    "--dataset_name=\"YOUR_DATASET_PATH\" \\\n",
    "--pretrained_vae_model_name_or_path=\"madebyollin/sdxl-vae-fp16-fix\" \\\n",
    "--output_dir=\"lora-trained-xl\" \\\n",
    "--instance_prompt=\"YOUR_TRIGGER_WORD person\" \\\n",
    "--resolution=1024 \\\n",
    "--train_batch_size=1 \\\n",
    "--gradient_accumulation_steps=4 \\\n",
    "--learning_rate=1e-4 \\\n",
    "--lr_scheduler=\"constant\" \\\n",
    "--lr_warmup_steps=0 \\\n",
    "--max_train_steps=400 \\\n",
    "--validation_prompt=\"A photo of YOUR_TRIGGER_WORD person in a studio\" \\\n",
    "--validation_epochs=5 \\\n",
    "--seed=\"0\" \\\n",
    "--hub_model_id=\"YOUR_HF_REPO_NAME\" \\\n",
    "--push_to_hub \\\n",
    "--mixed_precision=\"fp16\" \\\n",
    "--gradient_checkpointing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Inference\n",
    "\n",
    "After training, generate images with your fine-tuned model:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import gc\n",
    "import torch\n",
    "from diffusers import DiffusionPipeline\n",
    "from IPython.display import display, Image\n",
    "\n",
    "# Clear CUDA cache\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "# Load base model and LoRA weights\n",
    "\n",
    "pipeline = DiffusionPipeline.from_pretrained(\n",
    "\"stabilityai/stable-diffusion-xl-base-1.0\",\n",
    "torch_dtype=torch.float16,\n",
    ").to(\"cuda\")\n",
    "\n",
    "pipeline.load_lora_weights(\n",
    "\"YOUR_MODEL_PATH_ON_HF\" # Replace with your model path\n",
    ")\n",
    "\n",
    "# Generate image\n",
    "\n",
    "prompt = \"A photo of YOUR_TRIGGER_WORD person in a professional setting\"\n",
    "negative_prompt = \"blurry, low quality, distorted\"\n",
    "\n",
    "image = pipeline(\n",
    "prompt=prompt,\n",
    "negative_prompt=negative_prompt,\n",
    "num_inference_steps=40,\n",
    "guidance_scale=2.5,\n",
    ").images[0]\n",
    "\n",
    "# Display and save\n",
    "\n",
    "display(image)\n",
    "image.save(\"generated_image.png\")\n",
    "\n",
    "# Cleanup\n",
    "\n",
    "del pipeline\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Memory-Efficient Inference\n",
    "\n",
    "For systems with limited GPU memory:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import gc\n",
    "import torch\n",
    "from diffusers import AutoPipelineForText2Image\n",
    "\n",
    "# Clear cache\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "# Initialize with memory optimizations\n",
    "\n",
    "pipeline = AutoPipelineForText2Image.from_pretrained(\n",
    "\"stabilityai/stable-diffusion-xl-base-1.0\",\n",
    "torch_dtype=torch.float16,\n",
    ").to(\"cuda\")\n",
    "\n",
    "pipeline.enable_sequential_cpu_offload()\n",
    "pipeline.enable_attention_slicing(1)\n",
    "\n",
    "# Load LoRA weights\n",
    "\n",
    "pipeline.load_lora_weights(\n",
    "\"YOUR_MODEL_PATH_ON_HF\"\n",
    ")\n",
    "\n",
    "# Generate\n",
    "\n",
    "with torch.cuda.amp.autocast():\n",
    "image = pipeline(\n",
    "prompt=\"YOUR_PROMPT_HERE\",\n",
    "num_inference_steps=35,\n",
    "height=512,\n",
    "width=512,\n",
    "guidance_scale=2.5,\n",
    ").images[0]\n",
    "\n",
    "image.save(\"optimized_generation.png\")\n",
    "display(image)\n",
    "\n",
    "# Cleanup\n",
    "\n",
    "del pipeline\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interactive Gradio Interface (Optional)\n",
    "\n",
    "Create a user-friendly interface for generation:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "import torch\n",
    "from diffusers import AutoPipelineForText2Image\n",
    "import gc\n",
    "\n",
    "\n",
    "class ModelPipeline:\n",
    "    def __init__(self):\n",
    "        self.pipeline = None\n",
    "\n",
    "    def load_model(self):\n",
    "        if self.pipeline is None:\n",
    "            self.pipeline = AutoPipelineForText2Image.from_pretrained(\n",
    "                \"stabilityai/stable-diffusion-xl-base-1.0\",\n",
    "                torch_dtype=torch.float16,\n",
    "            ).to(\"cuda\")\n",
    "            self.pipeline.load_lora_weights(\"YOUR_MODEL_PATH_ON_HF\")\n",
    "            self.pipeline.enable_sequential_cpu_offload()\n",
    "            self.pipeline.enable_attention_slicing(1)\n",
    "\n",
    "    def generate_image(\n",
    "        self,\n",
    "        prompt,\n",
    "        negative_prompt=\"\",\n",
    "        num_steps=35,\n",
    "        guidance_scale=2.5,\n",
    "        width=512,\n",
    "        height=512,\n",
    "    ):\n",
    "        self.load_model()\n",
    "        with torch.cuda.amp.autocast():\n",
    "            image = self.pipeline(\n",
    "                prompt=prompt,\n",
    "                negative_prompt=negative_prompt,\n",
    "                num_inference_steps=num_steps,\n",
    "                guidance_scale=guidance_scale,\n",
    "                width=width,\n",
    "                height=height,\n",
    "            ).images[0]\n",
    "        return image\n",
    "\n",
    "\n",
    "# Initialize model\n",
    "model = ModelPipeline()\n",
    "\n",
    "\n",
    "# Create interface\n",
    "def generate(prompt, negative_prompt, steps, guidance, width, height):\n",
    "    return model.generate_image(\n",
    "        prompt, negative_prompt, int(steps), float(guidance), int(width), int(height)\n",
    "    )\n",
    "\n",
    "\n",
    "interface = gr.Interface(\n",
    "    fn=generate,\n",
    "    inputs=[\n",
    "        gr.Textbox(label=\"Prompt\"),\n",
    "        gr.Textbox(label=\"Negative Prompt\", value=\"\"),\n",
    "        gr.Slider(minimum=20, maximum=100, value=35, label=\"Steps\"),\n",
    "        gr.Slider(minimum=1.0, maximum=20.0, value=2.5, label=\"Guidance Scale\"),\n",
    "        gr.Slider(minimum=256, maximum=1024, value=512, step=64, label=\"Width\"),\n",
    "        gr.Slider(minimum=256, maximum=1024, value=512, step=64, label=\"Height\"),\n",
    "    ],\n",
    "    outputs=gr.Image(type=\"pil\"),\n",
    "    title=\"Fine-tuned SDXL Image Generator\",\n",
    "    description=\"Generate images using your fine-tuned SDXL model\",\n",
    ")\n",
    "\n",
    "\n",
    "interface.launch(share=True)  # Set share=False if you don't want a public URL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (WIP) Image Refinement\n",
    "\n",
    "### This section demonstrates an experimental feature to refine and improve already generated images. Note that this feature is still under development and results may vary.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import torch\n",
    "from diffusers import AutoPipelineForImage2Image\n",
    "from PIL import Image\n",
    "\n",
    "# Clear CUDA cache before starting\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "# Initialize the refinement pipeline\n",
    "pipeline = AutoPipelineForImage2Image.from_pretrained(\n",
    "    \"stabilityai/stable-diffusion-xl-refiner-1.0\",\n",
    "    torch_dtype=torch.float16,\n",
    "    use_safetensors=True,\n",
    ").to(\"cuda\")\n",
    "\n",
    "# Enable memory optimizations\n",
    "pipeline.enable_sequential_cpu_offload()\n",
    "pipeline.enable_attention_slicing(1)\n",
    "\n",
    "# Load an image to refine\n",
    "original_image = Image.open(\"your_generated_image.png\")\n",
    "if original_image.mode != \"RGB\":\n",
    "    original_image = original_image.convert(\"RGB\")\n",
    "\n",
    "# Refinement prompt - focus on aspects to enhance\n",
    "refinement_prompt = \"\"\"Ultra high quality photograph, perfect lighting, \n",
    "    no artifacts, clear details, professional photography\"\"\"\n",
    "\n",
    "# Generate refined image\n",
    "with torch.cuda.amp.autocast():\n",
    "    refined_image = pipeline(\n",
    "        prompt=refinement_prompt,\n",
    "        image=original_image,\n",
    "        strength=0.3,  # Adjust this value to control refinement intensity\n",
    "        num_inference_steps=30,\n",
    "        guidance_scale=7.5,\n",
    "    ).images[0]\n",
    "\n",
    "# Save the refined image\n",
    "refined_image.save(\"refined_image.png\")\n",
    "\n",
    "# Display before/after comparison\n",
    "display(original_image)\n",
    "print(\"Original Image ↑\")\n",
    "display(refined_image)\n",
    "print(\"Refined Image ↑\")\n",
    "\n",
    "# Clean up\n",
    "del pipeline\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes on Image Refinement:\n",
    "\n",
    "### This is a work in progress feature, at its current stage it did not seem to produce results that would beat the original image generation pipeline.\n",
    "\n",
    "### The strength parameter controls refinement intensity:\n",
    "\n",
    "<ul>\n",
    "<li>Lower values (0.1-0.3): Subtle improvements</li>\n",
    "<li>Higher values (0.4-0.7): More dramatic changes</li>\n",
    "</ul>\n",
    "\n",
    "Choose refinement prompts that focus on specific aspects you want to improve\n",
    "Results may vary depending on input image and prompt\n",
    "Keep the original image until you're satisfied with the refinement\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
